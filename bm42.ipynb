{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "from pydatrie import DoubleArrayTrie\n",
    "from elasticsearch import Elasticsearch\n",
    "from transformers import AutoTokenizer, AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"cl-nagoya/ruri-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Elasticsearch(\"http://localhost:9200/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kl/fr_z14214psgvtywhz6p9d5m0000gs/T/ipykernel_88827/3541529391.py:1: DeprecationWarning: Parameter dict_type of Dictionary() is deprecated, use dict instead\n",
      "  tokenizer_obj = dictionary.Dictionary(config_path=\"./sudachi.json\", dict_type=\"core\").create()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer_obj = dictionary.Dictionary(config_path=\"./sudachi.json\", dict_type=\"core\").create()  \n",
    "mode = tokenizer.Tokenizer.SplitMode.C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_attentions(text) -> dict[str, float]:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "    attentions = outputs.attentions[-1][0, :, 0].mean(dim=0)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "    token_attentions = {}\n",
    "    current_word = \"\"\n",
    "    current_weight = 0\n",
    "\n",
    "    for token, weight in zip(tokens[1:-1], attentions[1:-1]):\n",
    "        if token.startswith(\"##\"):\n",
    "            current_word += token[2:]\n",
    "            current_weight += float(weight)\n",
    "            continue\n",
    "\n",
    "        if current_word:\n",
    "            token_attentions[current_word] = current_weight\n",
    "        current_word = token\n",
    "        current_weight = float(weight.item())\n",
    "\n",
    "    if current_word:\n",
    "        token_attentions[current_word] = float(current_weight)\n",
    "\n",
    "    return token_attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retokenize_with_sudachi(tokens, text):\n",
    "    \"\"\"\n",
    "    Sudachiの形態素解析結果を元に、トークン化を結合する\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer_obj = dictionary.Dictionary(config_path=\"./sudachi.json\", dict_type=\"core\").create()  \n",
    "    sudachi_tokens = [m.surface() for m in tokenizer_obj.tokenize(text, mode)]\n",
    "\n",
    "    result = {}\n",
    "    for token in sudachi_tokens:\n",
    "        result[token] = 0\n",
    "        for t in tokens:\n",
    "            if t in token:\n",
    "                result[token] += float(tokens[t])\n",
    "\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ランキング: 0.2108\n",
      "qdrant: 0.1261\n",
      "試し: 0.1148\n",
      "アルゴリズム: 0.1063\n",
      "42: 0.0750\n",
      "ます: 0.0523\n",
      "を: 0.0492\n",
      "。: 0.0436\n",
      "BM: 0.0301\n",
      "ある: 0.0272\n",
      "新しい: 0.0171\n",
      "開発: 0.0149\n",
      "で: 0.0058\n",
      "た: 0.0056\n",
      "が: 0.0054\n",
      "し: 0.0051\n"
     ]
    }
   ],
   "source": [
    "result = get_token_attentions(\"qdrantが開発した新しいランキングアルゴリズムであるBM42を試します。\")\n",
    "for k, v in sorted(result.items(), key = lambda item : item[1], reverse=True):\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "併用: 0.1649\n",
      "半夏: 0.1020\n",
      "湯: 0.0594\n",
      "厚朴: 0.0523\n",
      "柴胡: 0.0436\n",
      "牡蛎: 0.0323\n",
      "竜骨: 0.0310\n",
      "加: 0.0213\n",
      "の: 0.0156\n",
      "と: 0.0149\n",
      "----------\n",
      "半夏厚朴湯: 0.2138\n",
      "柴胡加竜骨牡蛎湯: 0.1876\n",
      "併用: 0.1649\n",
      "の: 0.0156\n",
      "と: 0.0149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kl/fr_z14214psgvtywhz6p9d5m0000gs/T/ipykernel_88827/2843030033.py:6: DeprecationWarning: Parameter dict_type of Dictionary() is deprecated, use dict instead\n",
      "  tokenizer_obj = dictionary.Dictionary(config_path=\"./sudachi.json\", dict_type=\"core\").create()\n"
     ]
    }
   ],
   "source": [
    "example_text = \"半夏厚朴湯と柴胡加竜骨牡蛎湯の併用\"\n",
    "result = get_token_attentions(example_text)\n",
    "for k, v in sorted(result.items(), key = lambda item : item[1], reverse=True):\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"----------\")\n",
    "result = retokenize_with_sudachi(result, example_text)\n",
    "for k, v in sorted(result.items(), key = lambda item : item[1], reverse=True):\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = DoubleArrayTrie(\n",
    "    {\n",
    "        \"ばね指\": \"弾発指\",\n",
    "        \"弾発指\": \"ばね指\"\n",
    "    }\n",
    ")\n",
    "\n",
    "def token_expantion(tokens) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    トークンの類義語を追加する\n",
    "    \"\"\"\n",
    "\n",
    "    result = {}\n",
    "    for k, v in tokens.items():\n",
    "        result[k] = v\n",
    "        syn = synonyms.get(k)\n",
    "        if syn is not None:\n",
    "            result[syn] = v\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ばね指: 0.5203\n",
      "症状: 0.1462\n",
      "の: 0.0684\n",
      "に: 0.0675\n",
      "つい: 0.0506\n",
      "て: 0.0496\n",
      "----------\n",
      "ばね指: 0.5203\n",
      "症状: 0.1462\n",
      "の: 0.0684\n",
      "に: 0.0675\n",
      "つい: 0.0506\n",
      "て: 0.0496\n",
      "----------\n",
      "ばね指: 0.5203\n",
      "弾発指: 0.5203\n",
      "症状: 0.1462\n",
      "の: 0.0684\n",
      "に: 0.0675\n",
      "つい: 0.0506\n",
      "て: 0.0496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kl/fr_z14214psgvtywhz6p9d5m0000gs/T/ipykernel_88827/2843030033.py:6: DeprecationWarning: Parameter dict_type of Dictionary() is deprecated, use dict instead\n",
      "  tokenizer_obj = dictionary.Dictionary(config_path=\"./sudachi.json\", dict_type=\"core\").create()\n"
     ]
    }
   ],
   "source": [
    "example_text = \"ばね指の症状について\"\n",
    "result = get_token_attentions(example_text)\n",
    "for k, v in sorted(result.items(), key = lambda item : item[1], reverse=True):\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"----------\")\n",
    "result = retokenize_with_sudachi(result, example_text)\n",
    "for k, v in sorted(result.items(), key = lambda item : item[1], reverse=True):\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"----------\")\n",
    "result = token_expantion(result)\n",
    "for k, v in sorted(result.items(), key = lambda item : item[1], reverse=True):\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ランキング: 0.2108\n",
      "qdrant: 0.1261\n",
      "試し: 0.1148\n",
      "アルゴリズム: 0.1063\n",
      "42: 0.0750\n",
      "ます: 0.0523\n",
      "を: 0.0492\n",
      "。: 0.0436\n",
      "BM: 0.0301\n",
      "ある: 0.0272\n",
      "新しい: 0.0171\n",
      "開発: 0.0149\n",
      "で: 0.0058\n",
      "た: 0.0056\n",
      "が: 0.0054\n",
      "し: 0.0051\n"
     ]
    }
   ],
   "source": [
    "example_text = \"qdrantが開発した新しいランキングアルゴリズムであるBM42を試します。\"\n",
    "result = get_token_attentions(example_text)\n",
    "for k, v in sorted(result.items(), key = lambda item : item[1], reverse=True):\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"qdrantが開発した新しいランキングアルゴリズムであるBM42を試します。\",\n",
    "    \"検索ランキングで使われるBM25とは？\",\n",
    "    \"局所麻酔(キシロカイン)アレルギー及び迷走神経反射について\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in enumerate(texts):\n",
    "    tokens = get_token_attentions(t)\n",
    "    joined_text = ' '.join(tokens.keys())\n",
    "    doc = {\n",
    "        \"title\": t,\n",
    "        \"joined_tokens\": joined_text,\n",
    "        \"tokens\": tokens\n",
    "    }\n",
    "    resp = client.index(index=\"test-index\", id=i+1, document=doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "curl -X GET \\\n",
    "  http://localhost:9200/test-index/_explain/2 \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -H 'cache-control: no-cache' \\\n",
    "  -d '{\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"should\": [\n",
    "                {\n",
    "                    \"script_score\": {\n",
    "                        \"query\": {\n",
    "                            \"bool\": {\n",
    "                                \"filter\": {\n",
    "                                    \"match\": {\n",
    "                                        \"joined_tokens\": \"BM\"\n",
    "                                    }\n",
    "                                },\n",
    "                                \"should\": [\n",
    "                                    {\n",
    "                                        \"term\": {\n",
    "                                            \"tokens\": {\n",
    "                                                \"value\": \"BM\"\n",
    "                                            }\n",
    "                                        }\n",
    "                                    }\n",
    "                                ]\n",
    "                            }\n",
    "                        },\n",
    "                        \"script\": {\n",
    "                            \"source\": \"return _score / _termStats.docFreq().getSum() \"\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"script_score\": {\n",
    "                        \"query\": {\n",
    "                            \"bool\": {\n",
    "                                \"filter\": {\n",
    "                                    \"match\": {\n",
    "                                        \"joined_tokens\": \"検索\"\n",
    "                                    }\n",
    "                                },\n",
    "                                \"should\": [\n",
    "                                    {\n",
    "                                        \"term\": {\n",
    "                                            \"tokens\": {\n",
    "                                                \"value\": \"検索\"\n",
    "                                            }\n",
    "                                        }\n",
    "                                    }\n",
    "                                ]\n",
    "                            }\n",
    "                        },\n",
    "                        \"script\": {\n",
    "                            \"source\": \"return _score / _termStats.docFreq().getSum() \"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
